# LangChain Streamlit Chat with Memory

This repository demonstrates how to build a chatbot using **LangChain** and **Streamlit**, incorporating chat history memory for contextual responses. 

## ğŸš€ Features
- ğŸ—£ï¸ Conversational chatbot using LangChain
- ğŸ’¾ Chat memory to maintain context across user interactions
- ğŸ¨ Simple and interactive Streamlit UI
- ğŸ”Œ Easily extensible for various applications

## ğŸ“‚ Repository Structure
ğŸ“¦ Langchain_Streamlit_Chat_Memory â”œâ”€â”€ ğŸ“œ main.py # Main Streamlit app â”œâ”€â”€ ğŸ“œ chatbot.py # Chatbot logic using LangChain â”œâ”€â”€ ğŸ“œ memory.py # Chat memory management â”œâ”€â”€ ğŸ“œ requirements.txt # Dependencies â””â”€â”€ ğŸ“œ README.md # This document

bash
Copy
Edit

## ğŸ› ï¸ Installation & Setup

1. **Clone the Repository**
   ```bash
   git clone https://github.com/matusalemcassim/Langchain_Streamlit_Chat_Memory.git
   cd Langchain_Streamlit_Chat_Memory
   ```
   
2. **Create a Virtual Environment (Optional)**
  ```bash
  python -m venv venv
  source venv/bin/activate  # Mac/Linux
  venv\Scripts\activate     # Windows
  ```
3. Install Dependencies
Once inside the project directory, install all required dependencies:
```bash
  pip install -r requirements.txt
```
4. Run the Chatbot
Launch the Streamlit chatbot application by running:
  ```bash
streamlit run main.py
  ```

âš™ï¸ Configuration
- Ensure main.py and chatbot.py are properly configured for your use case.
- Modify memory.py if you want to customize how chat history is stored.
  
ğŸ“Œ Usage
- Start the chatbot by running streamlit run main.py.
- Type a message in the chat input.
- The chatbot will remember past interactions and provide responses with context.
  
ğŸ› ï¸ Customization
- You can modify chatbot.py to integrate different LLM models.
- Adjust memory.py for various chat memory strategies.
  
ğŸ¤ Contributions

Feel free to fork this repository, submit issues, or create pull requests!

ğŸ“œ License

This project is licensed under the MIT License.
